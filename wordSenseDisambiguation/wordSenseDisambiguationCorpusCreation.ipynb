{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading the wikipedia french dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset \n",
    "import pandas as pd \n",
    "data =pd.read_csv(\"../dataset/frenchSentences.csv\")\n",
    "data=data[\"fr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the dataset into a list to manage it easily \n",
    "dataCopy=list(data)\n",
    "sentencesList=[]\n",
    "for dataSentence in dataCopy : \n",
    "    if(len(str(dataSentence).split(\" \"))>6):\n",
    "        sentencesList.append(dataSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\INFOKOM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\INFOKOM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\INFOKOM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenizeSentence(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return tokens\n",
    "\n",
    "import string\n",
    "\n",
    "def removePunctuation(sentence):\n",
    "    # Create a translation table that maps each punctuation mark to None\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # Translate the sentence using the translation table\n",
    "    no_punct = sentence.translate(translator)\n",
    "    return no_punct\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def removeFrenchStopwords(sentence):\n",
    "    from nltk.corpus import stopwords\n",
    "    # Define your list of prepositions and pronouns\n",
    "    prepositions_et_pronoms = [\n",
    "        # Pronoms personnels sujets\n",
    "        \"je\", \"tu\", \"il\", \"elle\", \"nous\", \"vous\", \"ils\", \"elles\",\n",
    "        \n",
    "        # Pronoms personnels compléments\n",
    "        \"me\", \"te\", \"se\", \"le\", \"la\", \"lui\", \"nous\", \"vous\", \"les\", \"leur\",\n",
    "        \n",
    "        # Pronoms possessifs\n",
    "        \"mon\", \"ma\", \"mes\", \"ton\", \"ta\", \"tes\", \"son\", \"sa\", \"ses\", \"notre\", \n",
    "        \"notres\", \"nos\", \"votre\", \"votres\", \"vos\", \"leur\", \"leurs\",\n",
    "        \n",
    "        # Pronoms démonstratifs\n",
    "        \"ce\", \"cet\", \"cette\", \"ces\",\n",
    "        \n",
    "        # Pronoms indéfinis\n",
    "        \"quelqu'un\", \"quelque chose\", \"tout\", \"tous\", \"toute\", \"toutes\", \n",
    "        \"personne\", \"rien\", \"autre\", \"chacun\", \"nul\", \"certains\", \"plusieurs\",\n",
    "        \n",
    "        # Pronoms relatifs\n",
    "        \"qui\", \"que\", \"quoi\", \"dont\", \"où\", \"lequel\", \"laquelle\", \"lesquels\", \"lesquelles\",\n",
    "        \n",
    "        # Conjonctions de subordination\n",
    "        \"si\", \"que\", \"lorsque\", \"puisque\", \"quand\", \"bien que\", \"même si\",\n",
    "        \n",
    "        # Adverbes de liaison\n",
    "        \"donc\", \"ainsi\", \"par conséquent\", \"alors\",\n",
    "        \n",
    "        # Prépositions\n",
    "        \"dans\", \"sur\", \"sous\", \"entre\", \"par\", \"avec\", \"sans\", \"chez\", \"pour\", \"devant\", \n",
    "        \"derrière\", \"en\", \"vers\", \"contre\", \"à\", \"de\", \"depuis\", \"jusque\", \"envers\", \n",
    "        \"outre\", \"malgré\", \"en dehors de\"\n",
    "    ]\n",
    "    # Get the list of French stopwords\n",
    "    stop_words = set(stopwords.words(\"french\"))\n",
    "    stop_words.update(prepositions_et_pronoms)\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence, language='french')\n",
    "    # Remove stopwords\n",
    "    filtered_sentence = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "def cleanSentence(sentence):\n",
    "    sentence=removeFrenchStopwords(sentence)\n",
    "    sentence=removePunctuation(sentence)\n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Defining the annotation model using Gemini LLM\n",
    "import google.generativeai as genai\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "annotationsResult=[]\n",
    "# Set up the API key for Bard\n",
    "genai.configure(api_key=\"AIzaSyDAn5XmJsi6EJ5jZpO-VIuSrmOWrS4VD0Q\")\n",
    "\n",
    "# Set up the model with specific generation and safety settings\n",
    "generation_config = {\n",
    "    \"temperature\": 2.0,\n",
    "    \"top_p\": 0.5,\n",
    "    \"top_k\": 5,\n",
    "}\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", generation_config=generation_config)\n",
    "\n",
    "def write_output_to_csv(output_file, annotations):\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['sentence', 'Ambiguous Words'])\n",
    "        for annotation in annotations:\n",
    "            writer.writerow(annotation)\n",
    "\n",
    "def detect_ambiguous_words(prompt, sentencesList):\n",
    "    untreatedSentences=[]\n",
    "    annotations = []\n",
    "    convo = model.start_chat(history=[])\n",
    "    for i, sentence in enumerate(sentencesList):\n",
    "        sentence=(sentence)\n",
    "        retries = 3 # Maximum number of retries\n",
    "        backoff_time = 0.5  # Initial backoff time in seconds\n",
    "        print(\"working with sentence \"+str(i)+\" over \"+str(len(sentencesList)))\n",
    "        while retries > 0:\n",
    "            try:\n",
    "                \n",
    "                response = convo.send_message(f\"{sentence}\\n\\n{prompt}\")\n",
    "                result = response.text.strip().lower() if response and response.text else \"No response\"\n",
    "                while(result.find(\"json\")==-1):\n",
    "                    response = convo.send_message(f\"{sentence}\\n\\n{prompt}\")\n",
    "                    result = response.text.strip().lower() if response and response.text else \"No response\"\n",
    "                annotationsResult.append(result)\n",
    "                ambiguous_words = result\n",
    "                annotations.append([\" \".join(ambiguous_words)])\n",
    "                \n",
    "                print(result)\n",
    "                time.sleep(1)  # Random delay between requests\n",
    "                break  # Exit the retry loop if successful\n",
    "            except Exception as e:\n",
    "                #print(f\"Error processing sentence {i}: {e}. Retrying in {backoff_time} seconds...\")\n",
    "                time.sleep(backoff_time)\n",
    "                #print(e)\n",
    "                backoff_time += 0.5  \n",
    "                retries -= 1\n",
    "\n",
    "                if retries == 0:\n",
    "                    print(f\"Skipping sentence {i} due to safety concerns.\")\n",
    "                    print(e)\n",
    "                    untreatedSentences.append(i)\n",
    "              \n",
    "                \n",
    "    print(untreatedSentences)\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with sentence 0 over 21715840\n",
      "```json\n",
      "{\"sentence\": \"souvent considérée comme la plus ancienne des sciences, elle découle de notre étonnement et de nos questionnements envers le ciel l'astronomie est la science qui étudie l'univers au-delà de l'atmosphère terrestre.\", \"contextes\": \"astronomie\"}\n",
      "```\n",
      "working with sentence 1 over 21715840\n",
      "```json\n",
      "{\"sentence\": \"son nom vient du grec astron, qui veut dire étoile et nomos, qui veut dire loi.\", \"contextes\": \"étymologie\"}\n",
      "```\n",
      "working with sentence 2 over 21715840\n",
      "```json\n",
      "{\"sentence\": \"elle s'intéresse à des objets et des phénomènes tels que les étoiles, les planètes, les comètes, les galaxies et les propriétés de l'univers à grande échelle.\", \"contextes\": \"objet\"}\n",
      "```\n",
      "working with sentence 3 over 21715840\n",
      "```json\n",
      "{\"sentence\": \"plus spécifiquement, elle étudie la formation et l'évolution de l'univers, détermine les propriétés physiques et chimiques des objets célestes qu'il contient et mesure leurs déplacements.\", \"contextes\": \"etude\"}\n",
      "```\n",
      "working with sentence 4 over 21715840\n",
      "```json\n",
      "{\"sentence\": \"pour la majorité des gens, l'astronomie évoque un monde à la fois mystérieux et grandiose, celui des étoiles et de l'immensité cosmique.\", \"contextes\": \"perception\"}\n",
      "```\n",
      "working with sentence 5 over 21715840\n",
      "```json\n",
      "{\"sentence\": \"l'astronomie évoque donc aussi les grandes questions « existentielles » :\", \"contextes\": \"question\"}\n",
      "```\n",
      "working with sentence 6 over 21715840\n",
      "```json\n",
      "{\"sentence\": \"l'attrait exercé par ces énigmes universelles a représenté l'étincelle initiale du parcours de nombreux grands astronomes.\", \"contextes\": \"motivation\"}\n",
      "```\n",
      "working with sentence 7 over 21715840\n",
      "```json\n",
      "{\"sentence\": \"l'astronomie possède également une dimension beaucoup plus pratique et « terre à terre », laquelle est moins apparente aujourd'hui.\", \"contextes\": \"dimension\"}\n",
      "```\n",
      "working with sentence 8 over 21715840\n",
      "```json\n",
      "{\"sentence\": \"depuis toujours, nous observons les mouvements du ciel pour nous repérer dans l'espace et le temps.\", \"contextes\": \"repérage\"}\n",
      "```\n",
      "working with sentence 9 over 21715840\n",
      "```json\n",
      "{\"sentence\": \"ainsi, l'homme préhistorique connaît bien la relation existant entre la variation de la durée du jour et le cycle des saisons; il s'en sert pour planifier ses déplacements de chasse et de cueillette.\", \"contextes\": \"pratique\"}\n",
      "```\n",
      "working with sentence 10 over 21715840\n",
      "```json\n",
      "{\"sentence\": \"c'est aussi en observant la position des étoiles que les premiers agriculteurs décident du moment des grandes étapes de leur travail et que les navigateurs se repèrent sur les mers.\", \"contextes\": \"pratique\"}\n",
      "```\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m detect_ambiguous_words(prompt, sentencesList)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m      3\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massocier un topic a la phrase suivante et représentez-le comme \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontextes\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic de la phrase en un seul mot \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}.ignorez les propositions,les stop words ,les mots qui ne sont pas en francis et les mots qui ont ete deja traites \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_ambiguous_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentencesList\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 50\u001b[0m, in \u001b[0;36mdetect_ambiguous_words\u001b[1;34m(prompt, sentencesList)\u001b[0m\n\u001b[0;32m     47\u001b[0m     annotations\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ambiguous_words)])\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Random delay between requests\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Exit the retry loop if successful\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m#print(f\"Error processing sentence {i}: {e}. Retrying in {backoff_time} seconds...\")\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Detecting ambiguous words\n",
    "def main():\n",
    "    prompt = ('reperez le mot polysémique dans le texte suivant et représentez-le comme ''{\"sentence\": \"\", \"word\": \"\", \"senses\": [], \"contextes\":\"topic de la phrase\", \"senseexacte\": \"\",synonymes:\"tout les sens possibles\",rule:justifications linguistique}.ignorez les propositions,les stop words ,les mots qui ne sont pas en francis et les mots qui ont ete deja traites ')\n",
    "    annotations = detect_ambiguous_words(prompt, sentencesList[45200:])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detect_ambiguous_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m detect_ambiguous_words(prompt, sentencesList)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m      3\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreperez le named entity  dans le texte suivant et retournez-la comme sans explication \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msense\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnature grammaticale\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_ambiguous_words\u001b[49m(prompt, sentencesList)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'detect_ambiguous_words' is not defined"
     ]
    }
   ],
   "source": [
    "### Detecting concepts words\n",
    "def main():\n",
    "    prompt = ('reperez le named entity  dans le texte suivant et retournez-la comme sans explication ''{\"sentence\": \"\", \"word\": \"\", \"sense\":,\"nature grammaticale\":')\n",
    "    annotations = detect_ambiguous_words(prompt, sentencesList)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
